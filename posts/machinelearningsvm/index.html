<!DOCTYPE html><html lang="ko-KR" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Support Vector Machine, MATLAB으로 SVM 쪼아먹기" /><meta name="author" content="Simon Anderson" /><meta property="og:locale" content="ko_KR" /><meta name="description" content="0. Introduction" /><meta property="og:description" content="0. Introduction" /><link rel="canonical" href="https://simonwithwoogi.github.io/posts/machinelearningsvm/" /><meta property="og:url" content="https://simonwithwoogi.github.io/posts/machinelearningsvm/" /><meta property="og:site_name" content="몬기의 기술공방" /><meta property="og:image" content="https://simonwithwoogi.github.io/assets/img/MATLAB/10_Preview.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-01-30T02:20:00+09:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://simonwithwoogi.github.io/assets/img/MATLAB/10_Preview.png" /><meta property="twitter:title" content="Support Vector Machine, MATLAB으로 SVM 쪼아먹기" /><meta name="twitter:site" content="@no-name" /><meta name="twitter:creator" content="@Simon Anderson" /><meta name="google-site-verification" content="Simon Anderson verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Simon Anderson"},"description":"0. Introduction","url":"https://simonwithwoogi.github.io/posts/machinelearningsvm/","@type":"BlogPosting","image":"https://simonwithwoogi.github.io/assets/img/MATLAB/10_Preview.png","headline":"Support Vector Machine, MATLAB으로 SVM 쪼아먹기","dateModified":"2021-01-30T02:20:00+09:00","datePublished":"2021-01-30T02:20:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://simonwithwoogi.github.io/posts/machinelearningsvm/"},"@context":"https://schema.org"}</script><title>Support Vector Machine, MATLAB으로 SVM 쪼아먹기 | 몬기의 기술공방</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/Blog_logo_210213.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">몬기의 기술공방</a></div><div class="site-subtitle font-italic">Simon with Woogi</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/SimonWithWoogi" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/no-name" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['simon.anderson.tech','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Support Vector Machine, MATLAB으로 SVM 쪼아먹기</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Support Vector Machine, MATLAB으로 SVM 쪼아먹기</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Simon Anderson </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sat, Jan 30, 2021, 2:20 AM +0900" prep="on" > Jan 30 <i class="unloaded">2021-01-30T02:20:00+09:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4685 words">26 min</span></div></div><div class="post-content"><h2 id="0-introduction"><span style="color:darkblue">0. Introduction</span></h2><p><code class="language-plaintext highlighter-rouge">SVM(Support Vector Machine)</code> 은 맨 처음 <code class="language-plaintext highlighter-rouge">generalized portrait algorithm</code> 이라 불렸습니다. 그리고 1990년대 끝에서 <code class="language-plaintext highlighter-rouge">Kernel trick</code> 을 이용하여 <code class="language-plaintext highlighter-rouge">SVM</code> 에 적용한 논문, 더 나아가 <code class="language-plaintext highlighter-rouge">Soft margin</code> 을 이용한 <code class="language-plaintext highlighter-rouge">SVM</code> 이 나오면서 현재 <code class="language-plaintext highlighter-rouge">Machine learning</code> 에 사용하는 <code class="language-plaintext highlighter-rouge">SVM</code> 으로 굳어지게 됩니다. 그러니 이번 포스팅은 <code class="language-plaintext highlighter-rouge">Kernel method</code> 에 대한 설명으로 시작하여 <code class="language-plaintext highlighter-rouge">SVM</code> 을 이용한 <code class="language-plaintext highlighter-rouge">MATLAB</code> 실습으로 마무리합니다.</p><h2 id="1-kernel-trick"><span style="color:darkblue">1. Kernel Trick</span></h2><p><code class="language-plaintext highlighter-rouge">Kernel method</code> 는 <code class="language-plaintext highlighter-rouge">Memory</code> 기반 처리입니다. 학습이 끝나면 학습 데이터를 어느정도 저장하고 있다가 예측 단계에서 사용하죠. 사실 이 <code class="language-plaintext highlighter-rouge">Kernel method</code> 가 매우 강력하여 1990년대에 <code class="language-plaintext highlighter-rouge">Perceptron vs MachineLearning</code> 의 싸움에서 인공신경망인 <code class="language-plaintext highlighter-rouge">Perceptron</code> 을 깔끔하게 눌러버립니다. 결국 <code class="language-plaintext highlighter-rouge">Deep learning</code> 의 성능때문에 밀렸지만, 그 때 당시엔 인기였습니다. <strong>지금은 <code class="language-plaintext highlighter-rouge">Deep learning</code> 과 함께 어우러져 쓰고 있습니다.</strong></p><h3 id="11-trick-meaning"><span style="color:darkblue">1.1. Trick meaning</span></h3><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_1.png" alt="함정카드" /></p><p><strong>아니, 이런 학술적인 용어에 트릭이요?</strong></p><p>먼저, <code class="language-plaintext highlighter-rouge">운영체제(OS:Operating System)</code> 에서의 하드웨어와 중개자역할인 <code class="language-plaintext highlighter-rouge">Kernel</code> 과 개념적으로 유사하나 그에 대한 설명이 아닙니다. <code class="language-plaintext highlighter-rouge">Kernel Trick</code> 은 <strong>실제 차원에 대해서 공간을 변환하지 않고 우회하여 변환하는 효과를 주는 것입니다.</strong> 말그대로 <code class="language-plaintext highlighter-rouge">Trick</code> 이라서 <code class="language-plaintext highlighter-rouge">선형 분리</code> 가 불가능한 공간을 <code class="language-plaintext highlighter-rouge">선형 분리</code> 가 가능한 차원 혹은 공간 변환의 효과를 내는 것이죠. 우회를 자세히 얘기하면, 수리적인 방법으로 접근하여 변환된 공간을 매핑하지않고 내적을 얻는다고 말할 수 있습니다.</p><h3 id="12-kernel-function"><span style="color:darkblue">1.2. Kernel function</span></h3><p><code class="language-plaintext highlighter-rouge">ILSVRC(ImageNet Large Scale Visual Recognition Competition )</code> 에서 우승한 신경망 입력 사이즈를 확인해보면 보통 <code class="language-plaintext highlighter-rouge">224x224</code> 를 사용합니다. 사이즈를 따로 수정하지 않고 한 픽셀들이 곧 특징이고 차원으로 받아들일 수 있는데, 그렇다면 <code class="language-plaintext highlighter-rouge">50176</code> 차원이 나오게 됩니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_2.png" alt="XOR2to3" /></p><p>XOR의 문제는 3차원으로 변환하면서 구분이 가능하듯, 고차원을 통하면 분류에 있어서 이점은 확실히 있습니다. 그런데 <code class="language-plaintext highlighter-rouge">50176</code> 차원은 충분히 고차원이고 여기서 더 고차원으로 바꾸는 일은 쉽지 않습니다. 여기서는 <code class="language-plaintext highlighter-rouge">Kernel function</code> 이 차원변환에 도움을 줍니다. 일단 <code class="language-plaintext highlighter-rouge">Kernel function</code> 의 형태는 아래와 같습니다. x, z 가 특징 벡터라면 두 독립변수의 내적한 값을 가져온다는 얘기이고 이 <code class="language-plaintext highlighter-rouge">Kernel function</code> 의 핵심 또한 <strong>내적</strong> 을 구한다에 있습니다.</p>\[K(x, z) = \phi(x) \cdot \phi(z)\]<hr /><p><strong>유명한 세 가지 <code class="language-plaintext highlighter-rouge">Kernel function</code></strong></p>\[\begin{array}{l} \text{Polynomial Kernel : } K(x,z)\ =\ (x\cdot z +1)^p \\ \text{RBF Kernel : } K(x,z)\ =\ \exp(\frac{-||x-z||_2^2}{2\sigma^2}) \\ \text{Hyperbolic tangent Kernel : } = K(x,z)\ =\ \text{tanh}(\alpha x\cdot z + \beta) \end{array}\]<hr /><h3 id="13-core-points"><span style="color:darkblue">1.3. Core points </span></h3><p><code class="language-plaintext highlighter-rouge">Kernel function</code> 을 사용하면 내적을 구하는 연산량이 줄어들게 됩니다. 변환할 때의 매핑을 하지 않기 때문인데요. 기저함수를 이용하는 것보다 훨씬 효율적인 면이 있습니다. 결론적으로 <code class="language-plaintext highlighter-rouge">Kernel trick</code> 이란 매핑없이 변환 공간의 내적을 얻는 방법이고 이는 기존 공간 그대로 있으면서 선형적으로 분리 가능한 고차원 공간의 특성을 사용함을 의미합니다. 그래도 <code class="language-plaintext highlighter-rouge">Kernel trick</code> 이 만능은 아닙니다. 사용하려면 중요한 전제가 있는데요. 변환된 공간의 연산이 내적으로 표현되어야 합니다.</p><p>솔직히 <code class="language-plaintext highlighter-rouge">Kernel function</code> 에 대해서 많은 내용이 생략됐습니다. 왜 <code class="language-plaintext highlighter-rouge">Kernel</code> 이라 부르고 <code class="language-plaintext highlighter-rouge">Kernel</code> 이 무엇인가, 기존 고차원으로 변환할때는 연산이 얼마나 많이 걸릴까, 기저함수는 어떻게 정의할 수 있을까 등등, 그러나 이번 포스팅의 주제에 많이 벗어나는 것 같아 잘랐습니다. 왜냐하면 <code class="language-plaintext highlighter-rouge">Kernel trick</code> 을 <code class="language-plaintext highlighter-rouge">아주 성공적으로 이용한 사례인 SVM</code> 에 대한 느낌으로 얘기하는 것이 아니라 <code class="language-plaintext highlighter-rouge">SVM의 메커니즘을 이해하고 실습하기</code> 에 초점을 맞추고 싶었습니다.</p><h2 id="2-support-vector-machinesvm"><span style="color:darkblue">2. Support Vector Machine(SVM) </span></h2><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_3.png" alt="근육맛쿠키" /></p><p><code class="language-plaintext highlighter-rouge">Kernel trick</code> 이 붙은 <code class="language-plaintext highlighter-rouge">SVM</code> 은 더이상 무서울 것이 없습니다. 이건 뭐 그냥 깡패에요. <code class="language-plaintext highlighter-rouge">Kernel trick</code> 한번 적용됐다고 얘를 데리고 다니면서 처음엔 그저 <code class="language-plaintext highlighter-rouge">분류(classification)</code> 로만 쓰다가 <code class="language-plaintext highlighter-rouge">회귀(Regression)</code> 까지 지도 학습 모든 분야에 자리르 잡습니다.</p><h3 id="21-generalized-classfier"><span style="color:darkblue">2.1. Generalized classfier </span></h3><p><code class="language-plaintext highlighter-rouge">SVM</code> 은 처음 <code class="language-plaintext highlighter-rouge">generalized portrait algorithm</code> 이라 불렸던 만큼, 일반화 능력에 대해서 먼저 얘기를 하겠습니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_4.png" alt="분류일반화" /></p><p>위 산점도를 기준으로 1번은 실패한 분류기, 2번과 3번은 잘 분류된 분류기라 볼 수 있습니다. <strong>하지만 여기서 어떤 것이 새로운 데이터가 들어왔을 때 3번이 더 잘 분류할 것으로 보입니다.</strong> 왜냐하면 2번은 한쪽 집단에 너무 붙어있어서 불안하죠? 물론 데이터를 다 까보면 실제로 2번에 피팅됐을 수도 있습니다. 그러나 SVM의 핵심은 현재 가지고 있는 데이터에서 각 집단간 여백을 최대하 함에 있습니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_5.png" alt="그게아니죠" /></p><h3 id="22-linear-support-vector-machine"><span style="color:darkblue">2.2. Linear Support Vector Machine </span></h3><p>특정 선을 기준으로 집단을 나눌때, 그 선을 <code class="language-plaintext highlighter-rouge">결정경계(Decision boundary)</code> 라고 부르겠습니다. 먼저 집단이 2개라고 가정할때 <code class="language-plaintext highlighter-rouge">결정경계</code> 의 식은 아래와 같습니다.</p>\[d(x) = w^Tx+b=0 \\ w = \text{weight} \\ d(x) = \text{points in dimension}\]<p><code class="language-plaintext highlighter-rouge">경사하강법(Gradient Decent)</code> 에서, <code class="language-plaintext highlighter-rouge">Neural Network</code>, <code class="language-plaintext highlighter-rouge">Fully connected network</code> 에서 주로 보는 식입니다. <code class="language-plaintext highlighter-rouge">d(x) = 0</code> 으로 인하여, 양수면 A 집단, 음수면 B 집단으로 구분할 수 있습니다. 그리고 w는 기울기의 각도라면 b는 기울기의 위치를 결정합니다. 이제 b가 각 집단 여백의 딱 가운데에 있도록 조정합니다. <strong>그리고 그 여백의 딱 가운데에 있도록 도와주는 벡터를 Support Vector라고 합니다.</strong></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_6.png" alt="SV" /></p><h3 id="23-margin"><span style="color:darkblue">2.3. Margin </span></h3><p>이제는 <code class="language-plaintext highlighter-rouge">Margin(여백)</code> 이 가장 큰 기울기를 어떻게 계산하는 지 말씀드리겠습니다. w로 기울기 방향이 잘 잡혀있어야 기본적으로 좋은 모델일테니까요. 먼저 거리 계산식은 <code class="language-plaintext highlighter-rouge">L2 Norm</code> 을 이용합니다.</p>\[\begin{array}{l} L_2 &amp; = \sqrt {\sum_i^n x_i^2} \\ &amp; = \sqrt {x_1^2 + x_2^2 + x_3^2 + …. + x_n^2} \end{array}\]<p>x 를 <code class="language-plaintext highlighter-rouge">Support Vector</code> 라고 했을때, 여백은 아래의 수식으로 표현할 수 있습니다. 그리고 <code class="language-plaintext highlighter-rouge">d(x)가 음수면 A집단 / 양수면 B집단</code> 이라 했을 때 <code class="language-plaintext highlighter-rouge">d(x)</code> 를 1로 두었을 때 계산하기 쉬워 최종적으로 나오는 식에 참고해주세요.</p>\[\begin{array}{l} \text{Margin} &amp;= 2\cdot\frac{|d(x)|}{||w||_2}\\ &amp;= \frac{2}{||w||_2} \end{array}\]<p>이제 이 <code class="language-plaintext highlighter-rouge">Margin</code> 을 최대화할때, 여기저기 <code class="language-plaintext highlighter-rouge">Support Vector</code> 들을 바꿔가면서 어디가 제일 큰 여백일까 찾아보게 됩니다. 이를 <code class="language-plaintext highlighter-rouge">조건부 최적화(Conditional optimization)</code> 이라고 부르며 <code class="language-plaintext highlighter-rouge">라그랑주 승수(Lagurange multiplier)</code> 를 이용하여 해결합니다. 마지막으로 <code class="language-plaintext highlighter-rouge">Wolfe dual</code> 을 이용하여 내적이 나타나도록 수식을 바꿉니다. 수식 설명은 그리 어렵지 않지만 내용만 차지하기에 생략하겠습니다. 의미하는 바는 <code class="language-plaintext highlighter-rouge">w, b</code> 를 제거하고 <code class="language-plaintext highlighter-rouge">라그랑주 승수 a</code> 만 남아 간단한 연산이 됩니다.</p><h3 id="24-soft-margin"><span style="color:darkblue">2.4. Soft Margin </span></h3><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_7.png" alt="선형분리불가능" /></p><p>살다보면 선형분리가 불가능한 상황이 더 많습니다. <code class="language-plaintext highlighter-rouge">XOR</code> 문제만봐도 그렇지요. <code class="language-plaintext highlighter-rouge">선형 SVM</code> 은 선형분리가 가능한 2가지 분류로 가정했습니다. 그러다보니까 <code class="language-plaintext highlighter-rouge">Margin</code> 안에는 어떤 점도 찍히지 않았습니다. <strong><code class="language-plaintext highlighter-rouge">Soft margin</code> 은 선형분리가 안 되는 걸 받아들이고 <code class="language-plaintext highlighter-rouge">Margin</code> 안에 데이터를 허용하는 아이디어 입니다.</strong></p><hr /><p>존재 가능하며 고려할 필요있는 모든 데이터</p><ul><li><p>잘 분류됐으며 <code class="language-plaintext highlighter-rouge">결정경계 + 여백</code> 에 들어와있는 경우</p><li>오분류됐으며 <code class="language-plaintext highlighter-rouge">결정경계 + 여백</code> 에 들어와있는 경우<li>잘 분류됐으며 <code class="language-plaintext highlighter-rouge">여백</code> 밖에 있는 경우<li>오분류됐으며 <code class="language-plaintext highlighter-rouge">여백</code> 밖에 있는 경우</ul><hr /><p>위 네가지 경우를 하나의 수식으로 정리할 수 있습니다. <code class="language-plaintext highlighter-rouge">슬랙변수(Slack variable)</code> 를 추가하여 정리 가능합니다.</p>\[\begin{array}{l} \text{Slack variable} = \xi \\ 1-\xi \leq y(w^tx+b)\\ 1.\ \xi=0, \text{잘 분류된 여백밖 데이터}\\ 2.\ 0&lt;\xi\leq=1,\text{잘 분류된 여백 안 데이터}\\ 3.\ 1&lt;\xi,\text{오분류된 모든 데이터} \end{array}\]<p>이제 여백을 크게하면서 <code class="language-plaintext highlighter-rouge">슬랙변수</code> 가 0에 피팅하는 w를 찾으면 되겠습니다.</p>\[J(w,\xi)=\frac{1}{2}||w||^2_2+C\sum_{i=1}^{n}\xi_i \\ \text{첫째항은 여백을 크게, 둘째항은 슬랙변수가 0이 되게}\\ C=\text{hyper parameter}\]<p><code class="language-plaintext highlighter-rouge">Hyper parameter</code> 에 따라 데이터 정확도 혹은 오분류율에 얼마나 민감하게 반응할 지 조정할 수 있습니다.</p><p>이후로는 <code class="language-plaintext highlighter-rouge">라그랑주 승수</code> 로 변환하고, <code class="language-plaintext highlighter-rouge">Wolfe 쌍대 문제</code> 로 다시 작성합니다. 결론은 <code class="language-plaintext highlighter-rouge">2.3. Margin</code> 의 내용에서 C의 역할이 생겼다는 것으로 이해하시면 됩니다.</p><h3 id="25-non-linear-svm"><span style="color:darkblue">2.5. Non linear SVM </span></h3><p><code class="language-plaintext highlighter-rouge">비선형 SVM</code> 은 위에서 <code class="language-plaintext highlighter-rouge">Margin</code> 과 <code class="language-plaintext highlighter-rouge">선형 SVM</code> 을 설명했으니 그리 어렵지 않습니다. 여기서 <code class="language-plaintext highlighter-rouge">Kernel Trick</code> 을 이용하는데요. 다시한번 <code class="language-plaintext highlighter-rouge">Kernel function</code> 형태를 보겠습니다.</p>\[K(x, z) = \phi(x) \cdot \phi(z)\]<p>그리고 이번에는 <code class="language-plaintext highlighter-rouge">2.3. Margin</code> 에서 내적이 나타나도록 수식을 바꾼 <code class="language-plaintext highlighter-rouge">Wolfe dual</code> 의 식을 한번 꺼내보겠습니다.</p>\[\mathcal{L} = \sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i \alpha_j y_i y_j x_i \cdot x_j\\\]<p>이 수식은 L공간에서의 수식이고 이제 <code class="language-plaintext highlighter-rouge">Kernel trick</code> 형태에 맞게 H공간으로서 바꿔 작성하겠습니다.</p>\[\mathcal{L} = \sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i \alpha_j y_i y_j \phi(x_i) \cdot \phi(x_j)\\\]<p>빌드업이 너무 잘되어 있어서 <code class="language-plaintext highlighter-rouge">비선형 SVM</code> 은 쉽게 넘어갈 수 있겠습니다.</p><h3 id="26-c-classification-svm"><span style="color:darkblue">2.6. c-classification SVM </span></h3><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_8.png" alt="나루토분신술" /></p><p>지금까지 설명한 <code class="language-plaintext highlighter-rouge">SVM</code> 은 <code class="language-plaintext highlighter-rouge">Binomial classfication</code> 에 대한 이야기입니다. 두 집단에서 선형이냐, 비선형이냐로 싸운 것이죠. 근데 현실에서는 좀 더 여러 분류를 하다보니까 자주 쓰이는 <code class="language-plaintext highlighter-rouge">SVM</code> 은 <code class="language-plaintext highlighter-rouge">c-classification SVM</code> 입니다. 여러 종류의 기법이 있는데, 이에 대한 핵심은 <code class="language-plaintext highlighter-rouge">쌍</code> 을 통한 여백을 최대화 하는 것이므로 위에서 설명한 기존 <code class="language-plaintext highlighter-rouge">SVM</code> 을 복합적으로 활용한 사례입니다.</p><h3 id="27-svm-regression"><span style="color:darkblue">2.7. SVM regression </span></h3><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_9.png" alt="기존회귀와소프트마진" /></p><p>보통은 <code class="language-plaintext highlighter-rouge">regression</code> 을 설명하고 <code class="language-plaintext highlighter-rouge">classification</code> 을 설명합니다. 왜냐하면 대체적으로 <code class="language-plaintext highlighter-rouge">regression</code> 이 좀 더 단순하기도 하고 <code class="language-plaintext highlighter-rouge">classification</code> 이 어렵고 다음 스텝의 느낌이 들기 때문입니다. 그러나 <code class="language-plaintext highlighter-rouge">SVM</code> 은 반대입니다. <code class="language-plaintext highlighter-rouge">분류먼저, 회귀는 분류의 확장개념</code>이죠.</p><p>여기서는 <code class="language-plaintext highlighter-rouge">둔감오류함수(insensitive error function)</code> 를 얘기합니다. 왜냐하면 일반 회귀식에서는 차원을 올릴 기반이 없고, 내적 형태가 나와야하기 때문입니다. <code class="language-plaintext highlighter-rouge">둔감오류함수</code> 는 에러를 <code class="language-plaintext highlighter-rouge">Margin</code> 값에 따라서 값을 정해줍니다. \(\begin{array}{l} E_\epsilon(y_i-f(x_i)) = \begin{cases} 0\qquad &amp; \text{if}\quad|y_i-f(x_i)|&lt;\epsilon \\ |y_i-f(x_i)|&lt;\epsilon &amp;\text{otherwise} \end{cases} \end{array}\) <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_10.png" alt="둔감오류함수" /></p><p>저희는 이제 <code class="language-plaintext highlighter-rouge">슬랙변수</code> 도 <code class="language-plaintext highlighter-rouge">Kernel trick</code> 도 아니까 <code class="language-plaintext highlighter-rouge">라그랑주 승수</code> 를 넘어서 바로 예측 수식에 들어가겠습니다.</p>\[f(x)=\sum_{a_i\neq0,\hat{a_i}\neq0}(a_i-\hat{a_i})K(x,x_i)+b\]<p>여기서 주요점은 <code class="language-plaintext highlighter-rouge">summation</code> 은 아래조건입니다. <code class="language-plaintext highlighter-rouge">결정경계</code> 에서 잘 분류된 샘플은 들어올 필요없는 것다는 점이며, 즉 이 수식에는 <code class="language-plaintext highlighter-rouge">Support vector</code> 만 들어간다는 점이죠. 이제 <code class="language-plaintext highlighter-rouge">MATLAB</code> 실습을 진행하겠습니다.</p><h2 id="3-matlab"><span style="color:darkblue">3. MATLAB </span></h2><p><code class="language-plaintext highlighter-rouge">MATLAB</code> 에서 모든 <code class="language-plaintext highlighter-rouge">Machine learning</code> 함수명은 <code class="language-plaintext highlighter-rouge">fit</code> 이 붙습니다. 그리고 <code class="language-plaintext highlighter-rouge">분류(classification)</code> 은 <code class="language-plaintext highlighter-rouge">c</code> 가, <code class="language-plaintext highlighter-rouge">회귀(regression)</code> 은 <code class="language-plaintext highlighter-rouge">r</code> 이 붙습니다. 마지막으로 <code class="language-plaintext highlighter-rouge">method</code> 명이 끝에 붙어서 <code class="language-plaintext highlighter-rouge">머신러닝 / 분류 / 디시젼트리</code> 라고 한다면, 함수명은 <code class="language-plaintext highlighter-rouge">fitctree</code> 가 됩니다. 또 <code class="language-plaintext highlighter-rouge">머신러닝 / 분류 / knn</code> 는 <code class="language-plaintext highlighter-rouge">fitcknn</code> 이구요. <code class="language-plaintext highlighter-rouge">머신러닝 / 회귀 / 디시젼트리</code> 는 <code class="language-plaintext highlighter-rouge">fitrtree</code> 입니다.</p><p><strong>그런데 SVM은 예외가 있습니다. OneClass SVM과 MultiClass SVM을 따로 분류합니다.</strong> 그래서 OCSVM은 <code class="language-plaintext highlighter-rouge">fitcsvm</code> 으로, MCSVM은 <code class="language-plaintext highlighter-rouge">fitcecoc</code> 로 사용합니다. 더 나아가 고차원 데이터 세트에서 <code class="language-plaintext highlighter-rouge">이진분류</code>는 <code class="language-plaintext highlighter-rouge">fitclinear</code> 를 얘기하고 있습니다. 회귀는 <code class="language-plaintext highlighter-rouge">fitrsvm</code> 으로 묶입니다. 그러나 마찬가지로 고차원은 <code class="language-plaintext highlighter-rouge">fitrlinear</code> 를 사용합니다. 그리고 아래는 <code class="language-plaintext highlighter-rouge">이진 다중 저차원 분류에서 고차원 분류로</code> 설명을 드리겠습니다.</p><h3 id="31-one-class-svm"><span style="color:darkblue">3.1. One class SVM </span></h3><p>첫번째로, <code class="language-plaintext highlighter-rouge">단일 클래스(One Class)</code> 혹은 <code class="language-plaintext highlighter-rouge">이진 분류(Binomial classification)</code> 에 적합한 함수인 <code class="language-plaintext highlighter-rouge">fitcsvm</code> 에 대한 예제입니다. 또한 높은 차원에 대해서는 이 함수가 적합하지 않습니다. <code class="language-plaintext highlighter-rouge">이진 분류</code> 라고 해도 차원이 많다면, 다른 함수를 써야합니다.</p><p>[분류]</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="nb">load</span> <span class="n">fisheriris</span>
<span class="n">inds</span> <span class="o">=</span> <span class="o">~</span><span class="nb">strcmp</span><span class="p">(</span><span class="n">species</span><span class="p">,</span><span class="s1">'setosa'</span><span class="p">);</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">meas</span><span class="p">(</span><span class="n">inds</span><span class="p">,</span><span class="mi">3</span><span class="p">:</span><span class="mi">4</span><span class="p">);</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">species</span><span class="p">(</span><span class="n">inds</span><span class="p">);</span>

<span class="n">SVMModel</span> <span class="o">=</span> <span class="n">fitcsvm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">classOrder</span> <span class="o">=</span> <span class="n">SVMModel</span><span class="o">.</span><span class="n">ClassNames</span>

<span class="n">sv</span> <span class="o">=</span> <span class="n">SVMModel</span><span class="o">.</span><span class="n">SupportVectors</span><span class="p">;</span>
<span class="nb">figure</span>
<span class="n">gscatter</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="mi">1</span><span class="p">),</span><span class="n">X</span><span class="p">(:,</span><span class="mi">2</span><span class="p">),</span><span class="n">y</span><span class="p">)</span>
<span class="nb">hold</span> <span class="n">on</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">sv</span><span class="p">(:,</span><span class="mi">1</span><span class="p">),</span><span class="n">sv</span><span class="p">(:,</span><span class="mi">2</span><span class="p">),</span><span class="s1">'ko'</span><span class="p">,</span><span class="s1">'MarkerSize'</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">legend</span><span class="p">(</span><span class="s1">'versicolor'</span><span class="p">,</span><span class="s1">'virginica'</span><span class="p">,</span><span class="s1">'Support Vector'</span><span class="p">)</span>
<span class="nb">hold</span> <span class="n">off</span>
</pre></table></code></div></div><p>아무래도 <code class="language-plaintext highlighter-rouge">단일 클래스 분류</code> 라서, 분류할 두가지(<code class="language-plaintext highlighter-rouge">Versicolor</code>, <code class="language-plaintext highlighter-rouge">Verginica</code>)만 가져옵니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_11.png" alt="plot" /></p><p>[이상치, SupportVector 확인하기]</p><p><strong>2차 제작에 대한 출처 :</strong> <a href="https://kr.mathworks.com/help/stats/fitcsvm.html">mathworks help</a> , <a href="https://kr.mathworks.com/help/stats/examples/classification.html#d122e1480">fisheriris</a></p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="nb">load</span> <span class="n">fisheriris</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">meas</span><span class="p">(:,</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">);</span>
<span class="n">y</span> <span class="o">=</span> <span class="nb">ones</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="mi">1</span><span class="p">);</span>
</pre></table></code></div></div><p><code class="language-plaintext highlighter-rouge">fisher의 iris</code> 데이터를 가져옵니다.<code class="language-plaintext highlighter-rouge"> 피셔의 붓꽃 데이터의 meas</code>는 150개 붓꽃 표본의 꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비에 대한 측정값으로 구성되어 있습니다. 그러므로 <code class="language-plaintext highlighter-rouge">X</code> 에는 150개의 붓꽃 표본의 꽃받침 길이, 꽃받침 너비만 들어가 있습니다. 그리고 <code class="language-plaintext highlighter-rouge">Y</code> 는 150x1 짜리로 사이즈에 1로 구성된 열벡터가 들어가있습니다.</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="nb">rng</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="n">SVMModel</span> <span class="o">=</span> <span class="n">fitcsvm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="s1">'KernelScale'</span><span class="p">,</span><span class="s1">'auto'</span><span class="p">,</span><span class="s1">'Standardize'</span><span class="p">,</span><span class="nb">true</span><span class="p">,</span><span class="k">...</span>
    <span class="s1">'OutlierFraction'</span><span class="p">,</span><span class="mf">0.05</span><span class="p">);</span><span class="mi">0</span>
</pre></table></code></div></div><p>수정된 데이터 세트를 사용하여 SVM 분류기를 훈련시킵니다. 관측값의 5%가 이상값이라고 가정합니다. 예측 변수를 표준화합니다.</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="n">svInd</span> <span class="o">=</span> <span class="n">SVMModel</span><span class="o">.</span><span class="n">IsSupportVector</span><span class="p">;</span>
<span class="n">h</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">;</span> <span class="c1">% Mesh grid step size</span>
<span class="p">[</span><span class="n">X1</span><span class="p">,</span><span class="n">X2</span><span class="p">]</span> <span class="o">=</span> <span class="nb">meshgrid</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="mi">1</span><span class="p">)):</span><span class="n">h</span><span class="p">:</span><span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="mi">1</span><span class="p">)),</span><span class="k">...</span>
    <span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="mi">2</span><span class="p">)):</span><span class="n">h</span><span class="p">:</span><span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="mi">2</span><span class="p">)));</span>
<span class="p">[</span><span class="o">~</span><span class="p">,</span><span class="n">score</span><span class="p">]</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">SVMModel</span><span class="p">,[</span><span class="n">X1</span><span class="p">(:),</span><span class="n">X2</span><span class="p">(:)]);</span>
<span class="n">scoreGrid</span> <span class="o">=</span> <span class="nb">reshape</span><span class="p">(</span><span class="n">score</span><span class="p">,</span><span class="nb">size</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="nb">size</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span><span class="mi">2</span><span class="p">));</span>
</pre></table></code></div></div><p>훈련된 <code class="language-plaintext highlighter-rouge">SVMModel</code> 에서 시각화 준비를 합니다. 또한 <code class="language-plaintext highlighter-rouge">predict</code> 에서 분류된 레이블만이 아니라 유사도를 볼 수 있는 <code class="language-plaintext highlighter-rouge">score</code> 를 가져옵니다.</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="nb">figure</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">X</span><span class="p">(:,</span><span class="mi">1</span><span class="p">),</span><span class="n">X</span><span class="p">(:,</span><span class="mi">2</span><span class="p">),</span><span class="s1">'k.'</span><span class="p">)</span>
<span class="nb">hold</span> <span class="n">on</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">X</span><span class="p">(</span><span class="n">svInd</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">X</span><span class="p">(</span><span class="n">svInd</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="s1">'ro'</span><span class="p">,</span><span class="s1">'MarkerSize'</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">contour</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span><span class="n">X2</span><span class="p">,</span><span class="n">scoreGrid</span><span class="p">)</span>
<span class="nb">colorbar</span><span class="p">;</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'{\bf Iris Outlier Detection via One-Class SVM}'</span><span class="p">)</span>
<span class="nb">xlabel</span><span class="p">(</span><span class="s1">'Sepal Length (cm)'</span><span class="p">)</span>
<span class="nb">ylabel</span><span class="p">(</span><span class="s1">'Sepal Width (cm)'</span><span class="p">)</span>
<span class="nb">legend</span><span class="p">(</span><span class="s1">'Observation'</span><span class="p">,</span><span class="s1">'Support Vector'</span><span class="p">)</span>
<span class="nb">hold</span> <span class="n">off</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_12.png" alt="plot" /></p><p>이상값을 나머지 데이터와 분리하는 경계는 등고선 값이 <code class="language-plaintext highlighter-rouge">0</code>인 위치에서 나타납니다.</p><p>교차 검증된 데이터에서 음의 점수를 갖는 관측값의 비율이 5%에 가까운지 확인합니다.</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="n">CVSVMModel</span> <span class="o">=</span> <span class="n">crossval</span><span class="p">(</span><span class="n">SVMModel</span><span class="p">);</span>
<span class="p">[</span><span class="o">~</span><span class="p">,</span><span class="n">scorePred</span><span class="p">]</span> <span class="o">=</span> <span class="n">kfoldPredict</span><span class="p">(</span><span class="n">CVSVMModel</span><span class="p">);</span>
<span class="n">outlierRate</span> <span class="o">=</span> <span class="nb">mean</span><span class="p">(</span><span class="n">scorePred</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span>
</pre></table></code></div></div><p>실제로 설정한 5%와 가까운지 <code class="language-plaintext highlighter-rouge">outlierRate</code> 가 알려줍니다.</p><h3 id="32-multi-class-svm"><span style="color:darkblue">3.2. Multi class SVM </span></h3><p>두번째는 <code class="language-plaintext highlighter-rouge">Multi class SVM</code>, <code class="language-plaintext highlighter-rouge">다중 분류, c-분류 SVM</code> 에 대한 예제입니다. <code class="language-plaintext highlighter-rouge">fitcecoc</code> 를 사용하고, 저~중 차원의 데이터에 적합합니다.</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="nb">load</span> <span class="n">fisheriris</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">meas</span><span class="p">;</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">species</span><span class="p">;</span>
<span class="nb">rng</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span> <span class="c1">% For reproducibility</span>
</pre></table></code></div></div><p>이번에는 피셔데이터를 몽땅씁니다. <code class="language-plaintext highlighter-rouge">다중분류</code> 라서 두 개만 고를 이유가 없거든요.</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="n">t</span> <span class="o">=</span> <span class="n">templateSVM</span><span class="p">(</span><span class="s1">'Standardize'</span><span class="p">,</span><span class="nb">true</span><span class="p">)</span>
<span class="n">Mdl</span> <span class="o">=</span> <span class="n">fitcecoc</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="s1">'Learners'</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="k">...</span>
    <span class="s1">'ClassNames'</span><span class="p">,{</span><span class="s1">'setosa'</span><span class="p">,</span><span class="s1">'versicolor'</span><span class="p">,</span><span class="s1">'virginica'</span><span class="p">});</span>
</pre></table></code></div></div><p><code class="language-plaintext highlighter-rouge">templateSVM</code> 를 이용하여 <code class="language-plaintext highlighter-rouge">모델 옵션</code> 을 만들어줍니디. 그리고 <code class="language-plaintext highlighter-rouge">fitcecoc</code> 를 통해 <code class="language-plaintext highlighter-rouge">model</code> 을 만듭니다.</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">CVMdl</span> <span class="o">=</span> <span class="n">crossval</span><span class="p">(</span><span class="n">Mdl</span><span class="p">);</span>
<span class="n">genError</span> <span class="o">=</span> <span class="n">kfoldLoss</span><span class="p">(</span><span class="n">CVMdl</span><span class="p">)</span>
</pre></table></code></div></div><p><code class="language-plaintext highlighter-rouge">crossval</code> 은 교차검증이고 <code class="language-plaintext highlighter-rouge">kfoldLoss</code> 는 <code class="language-plaintext highlighter-rouge">k-fold</code> 검증입니다.둘 다 일반화된 분류 오차를 볼 수 있습니다.</p><h3 id="33-svm-for-high-dimension"><span style="color:darkblue">3.3. SVM for High dimension </span></h3><p>마지막으로 <code class="language-plaintext highlighter-rouge">CNN</code> 과 결합하여 사용할때 주로 고차원이 만들어지는데, 이 외에도 많은 변수들을 사용하여 예측할 때 쓰는 <code class="language-plaintext highlighter-rouge">fitclinear / fitrlinear</code> 입니다. 이 챕터는 번역된 자료가 없어서 제가 번역을 하여 2차 창작을 했습니다.</p><p><a href="https://kr.mathworks.com/help/stats/fitclinear.html">Mathworks help</a></p><h4 id="331-description"><span style="color:darkblue">3.3.1. description </span></h4><p><code class="language-plaintext highlighter-rouge">fitclinear</code> 는 선형 분류 모델로 <code class="language-plaintext highlighter-rouge">고차원 이진분류</code>, <code class="language-plaintext highlighter-rouge">많은 예측 데이터</code> , 정규화된 <code class="language-plaintext highlighter-rouge">SVM</code> 에서의 선형분리가 가능한 모델, <code class="language-plaintext highlighter-rouge">로지스틱 회귀</code> 모델에 사용합니다. <code class="language-plaintext highlighter-rouge">fitclinear</code> 는 <code class="language-plaintext highlighter-rouge">스토캐스틱 경사하강법</code> 같은 기술로서 적은 연산시간으로 목적함수 최소화를 진행합니다. 뭐 어쨌든 많은 변수가 있으면 고차원이고, 그 고차원에서는 아무래도 연산량이 늘어나다보니 <code class="language-plaintext highlighter-rouge">fitclinear</code> 는 고차원에서 생기는 느려지는 연산에 초점을 맞춘 함수입니다.</p><h4 id="332-train-linear-classification-model-"><span style="color:darkblue">3.3.2. Train Linear Classification Model </span></h4><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="nb">load</span> <span class="n">nlpdata</span>
<span class="n">Ystats</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">==</span> <span class="s1">'stats'</span><span class="p">;</span>
<span class="n">Lambda</span> <span class="o">=</span> <span class="nb">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">11</span><span class="p">);</span>
</pre></table></code></div></div><p><code class="language-plaintext highlighter-rouge">NLP dataset</code> 을 이용하구요 . X는 예측변수들에 대한 희소행렬입니다. Y는 종속변수이면서 레이블입니다. 그리고 <code class="language-plaintext highlighter-rouge">Lambda</code> 의 <code class="language-plaintext highlighter-rouge">logspace</code> 는 11개의 10^-6 에서 10^-0.5 사이의 로그간격 값을 만듭니다.</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">'</span><span class="p">;</span> 
<span class="nb">rng</span><span class="p">(</span><span class="mi">10</span><span class="p">);</span> <span class="c1">% For reproducibility</span>
<span class="n">CVMdl</span> <span class="o">=</span> <span class="n">fitclinear</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Ystats</span><span class="p">,</span><span class="s1">'ObservationsIn'</span><span class="p">,</span><span class="s1">'columns'</span><span class="p">,</span><span class="s1">'KFold'</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="k">...</span>
    <span class="s1">'Learner'</span><span class="p">,</span><span class="s1">'svm'</span><span class="p">,</span><span class="s1">'Solver'</span><span class="p">,</span><span class="s1">'dual'</span><span class="p">,</span><span class="s1">'Regularization'</span><span class="p">,</span><span class="s1">'ridge'</span><span class="p">,</span><span class="k">...</span>
    <span class="s1">'Lambda'</span><span class="p">,</span><span class="n">Lambda</span><span class="p">)</span>
</pre></table></code></div></div><p><code class="language-plaintext highlighter-rouge">CVMdl</code> 이라는 모델을 만들었구요. <code class="language-plaintext highlighter-rouge">Learner</code> 는 <code class="language-plaintext highlighter-rouge">svm</code> , <code class="language-plaintext highlighter-rouge">Solver</code> 는 <code class="language-plaintext highlighter-rouge">dual</code>, <code class="language-plaintext highlighter-rouge">Regulation</code> 은 <code class="language-plaintext highlighter-rouge">ridge</code> 로 설정합니다.</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>numCLModels = numel(CVMdl.Trained)
</pre></table></code></div></div><p>5겹 교차검증(<code class="language-plaintext highlighter-rouge">KFold</code>, 5) 로 설정해서 훈련된 <code class="language-plaintext highlighter-rouge">CVMdl</code> 부분 모델이 5개가 나왔나 확인해봅니다.</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">Mdl1</span> <span class="o">=</span> <span class="n">CVMdl</span><span class="o">.</span><span class="n">Trained</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span>
</pre></table></code></div></div><p>그 중 하나만 잡아와 특징을 확인하구요.</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">ce</span> <span class="o">=</span> <span class="n">kfoldLoss</span><span class="p">(</span><span class="n">CVMdl</span><span class="p">);</span>
</pre></table></code></div></div><p><code class="language-plaintext highlighter-rouge">cross validation error</code> 를 확인합니다.</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">Mdl</span> <span class="o">=</span> <span class="n">fitclinear</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Ystats</span><span class="p">,</span><span class="s1">'ObservationsIn'</span><span class="p">,</span><span class="s1">'columns'</span><span class="p">,</span><span class="k">...</span>
    <span class="s1">'Learner'</span><span class="p">,</span><span class="s1">'svm'</span><span class="p">,</span><span class="s1">'Solver'</span><span class="p">,</span><span class="s1">'dual'</span><span class="p">,</span><span class="s1">'Regularization'</span><span class="p">,</span><span class="s1">'ridge'</span><span class="p">,</span><span class="k">...</span>
    <span class="s1">'Lambda'</span><span class="p">,</span><span class="n">Lambda</span><span class="p">);</span>
<span class="n">numNZCoeff</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">Mdl</span><span class="o">.</span><span class="n">Beta</span><span class="o">~=</span><span class="mi">0</span><span class="p">);</span>
</pre></table></code></div></div><p>아까 봤던 것은 <code class="language-plaintext highlighter-rouge">K-fold</code> 옵션을 넣은 것이구요. 이번에는 빼보겠습니다.</p><div class="language-matlab highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="nb">figure</span><span class="p">;</span>
<span class="p">[</span><span class="n">h</span><span class="p">,</span><span class="n">hL1</span><span class="p">,</span><span class="n">hL2</span><span class="p">]</span> <span class="o">=</span> <span class="nb">plotyy</span><span class="p">(</span><span class="nb">log10</span><span class="p">(</span><span class="n">Lambda</span><span class="p">),</span><span class="nb">log10</span><span class="p">(</span><span class="n">ce</span><span class="p">),</span><span class="k">...</span>
    <span class="nb">log10</span><span class="p">(</span><span class="n">Lambda</span><span class="p">),</span><span class="nb">log10</span><span class="p">(</span><span class="n">numNZCoeff</span><span class="p">));</span> 
<span class="n">hL1</span><span class="o">.</span><span class="n">Marker</span> <span class="o">=</span> <span class="s1">'o'</span><span class="p">;</span>
<span class="n">hL2</span><span class="o">.</span><span class="n">Marker</span> <span class="o">=</span> <span class="s1">'o'</span><span class="p">;</span>
<span class="nb">ylabel</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="s1">'log_{10} classification error'</span><span class="p">)</span>
<span class="nb">ylabel</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span><span class="s1">'log_{10} nonzero-coefficient frequency'</span><span class="p">)</span>
<span class="nb">xlabel</span><span class="p">(</span><span class="s1">'log_{10} Lambda'</span><span class="p">)</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'Test-Sample Statistics'</span><span class="p">)</span>
<span class="nb">hold</span> <span class="n">off</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MATLAB/10_13.png" alt="plot" /></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/matlab/'>MATLAB</a>, <a href='/categories/applied/'>Applied</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/matlab/" class="post-tag no-text-decoration" >MATLAB</a> <a href="/tags/algorithm/" class="post-tag no-text-decoration" >Algorithm</a> <a href="/tags/machinelearning/" class="post-tag no-text-decoration" >MachineLearning</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/deeplearning/" class="post-tag no-text-decoration" >DeepLearning</a> <a href="/tags/optimization/" class="post-tag no-text-decoration" >Optimization</a> <a href="/tags/mathmetics/" class="post-tag no-text-decoration" >Mathmetics</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Support Vector Machine, MATLAB으로 SVM 쪼아먹기 - 몬기의 기술공방&url=https://simonwithwoogi.github.io/posts/machinelearningsvm/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Support Vector Machine, MATLAB으로 SVM 쪼아먹기 - 몬기의 기술공방&u=https://simonwithwoogi.github.io/posts/machinelearningsvm/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Support Vector Machine, MATLAB으로 SVM 쪼아먹기 - 몬기의 기술공방&url=https://simonwithwoogi.github.io/posts/machinelearningsvm/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Cpp_Trim/">C++_string_Trim</a><li><a href="/posts/Blazor_start/">Blazor_start</a><li><a href="/posts/projecthyundaithree/">Twitter Naver YouTube Crawler, 현대자동차 여론을 조사해보자(3)</a><li><a href="/posts/GitBlog2/">(깃블로그 운영-2) 테마를 다운받고 웹페이지를 확인해보자</a><li><a href="/posts/projecthyundaione/">Textmining, 현대자동차 여론을 조사해보자(1)</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/matlab/">MATLAB</a> <a class="post-tag" href="/tags/java/">java</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/deeplearning/">DeepLearning</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/statistics/">Statistics</a> <a class="post-tag" href="/tags/improvement/">Improvement</a> <a class="post-tag" href="/tags/optimization/">Optimization</a> <a class="post-tag" href="/tags/quality/">Quality</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/matlabastaralgo/"><div class="card-body"> <span class="timeago small" > Sep 24, 2020 <i class="unloaded">2020-09-24T18:30:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>MATLAB A* Algorithm을 실습하고 지도 데이터에 그리기</h3><div class="text-muted small"><p> 1. What is A* Algorithm 1.1. Informed Search Search, 탐색에 관하여 알아봅시다. 지금 가고 싶은 곳을 떠올려 봅시다. 그리고 현재위치에서 목적지까지 시간을 얼마나 걸릴까요? 비용은 얼마나 들까요? 또 어디를 거쳐가야 할까요? 인터넷에 검색해보면 빠르게 답을 찾을 수 있습니다. 그런데 사실 다른 방법으로...</p></div></div></a></div><div class="card"> <a href="/posts/matlabneuralnet1/"><div class="card-body"> <span class="timeago small" > Nov 4, 2020 <i class="unloaded">2020-11-04T00:07:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>MATLAB Neural Network를 이해하고 SLP MLP의 학습원리를 이해해보자(1)!</h3><div class="text-muted small"><p> 이번 포스팅에는 귀여운 그림이 전혀 없습니다. 1. Neural Network 사람은 감각기관에서 받아들이는 내용에 대해 왜곡이 일어날 때가 있습니다. 이미 가지고 있는 지식때문인데요. 예능 프로그램에 검은 상자 속에 어떤 물컹한 물체를 놔두고 출연자들이 촉감으로 이 물체가 무엇인지 맞추는 게임을 종종 보셨을 겁니다. 재밌죠. 대부분 소스라치게 놀...</p></div></div></a></div><div class="card"> <a href="/posts/matlabneuralnet2/"><div class="card-body"> <span class="timeago small" > Nov 5, 2020 <i class="unloaded">2020-11-05T20:49:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>MATLAB Neural Network를 이해하고 SLP MLP의 학습원리를 이해해보자(2)!</h3><div class="text-muted small"><p> 전편:Forward propagation 이번 포스팅에는 귀여운 그림이 전혀 없습니다. 1. Forward propagation [\begin{array}{l} h_1\ =\ w^1{11}x_1+w^1{21}x_2+c_1 h_2\ =\ w^1{12}x_1+w^1{22}x_2+c_2 \hat{h} = \hat{w}^{1’}\hat{x}+\ha...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/javaStart/" class="btn btn-outline-primary" prompt="Older"><p>JAVA를 시작해보자</p></a> <a href="/posts/inqualitysixsigma/" class="btn btn-outline-primary" prompt="Newer"><p>6 Sigma, 식스 시그마 2편</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://github.com/simonwithwoogi">Simon Anderson</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/matlab/">MATLAB</a> <a class="post-tag" href="/tags/java/">java</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/deeplearning/">DeepLearning</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/statistics/">Statistics</a> <a class="post-tag" href="/tags/improvement/">Improvement</a> <a class="post-tag" href="/tags/optimization/">Optimization</a> <a class="post-tag" href="/tags/quality/">Quality</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://SimonWithWoogi.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
